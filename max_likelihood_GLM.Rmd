---
title: "Maximum Likelihood and Generalized Linear Models"
subtitle: Hector 2015 Chapter 8
output:
  html_document:
    df_print: paged
---
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages_needed <- c("SemiPar", # data
                     "ggplot2", # graphics
                     "MASS", # Box-Cox
                     "arm", # display() etc.
                     "ggfortify" # check model assumptions
                     )
pk_to_install <- packages_needed [!( packages_needed %in% rownames(installed.packages())  )]
if(length(pk_to_install)>0 ){
  install.packages(pk_to_install,repos="http://cran.r-project.org")
}
#lapply(packages_needed, require, character.only = TRUE)
library("SemiPar")
library("ggplot2")
library("MASS")
library("arm")
library("ggfortify")
```
\
**What is Maximum Likelihood?**

* Given a set of data and statistical model, find the values of the parameters that are most likely to reproduce the data; those that "find the best fit"

* Remember that we have seen this before in a different context; Ordinary Least Squares (OLS) Regression finds the single-best line of fit that minimizes the sum of the squared differences; works for normal data

* OLS doesn't work with data having other distributions; binomial is obvious example

* Maximum likelihood provides a more general approach that applies to a broader range of data (hence 'generalized')

* The method that underlies GLMs is called 'iterative weighted least squares'

* Where OLS has exactly one best solution for model fit, ML methods are *iterative* and *approximate*. Essentially we try out many potential lines of best fit using algoriths and gradually home in on what looks like the best solution.

* To avoid very small numbers the calculations are done with the (natural) log-likelihood (calculations that are multiplicative on the likelihood scale become additive with log-likelihoods).

* The generalized version of SS is called the **deviance**. Deviance is defined in terms of comparing pairs of models where one is more complex and the other a simplified version (as done in R using the `anova()` function).

* Deviance is twice the difference in the log-likelihoods of the more complex and simplified models (expressed as a negative value). 

* Deviance approximately follows the chi-square distribution (with DF equal to the number of parameters of the model), which allows the calculation of levels of probability and confidence, just as the normal distribution does when using normal least squares.

```{r janka linear model}
data(janka) # load data
names(janka) <- c("Density", "Hardness") # assign full names
janka # Box 4.1
#Timber hardness measured on the 'Janka' scale
ggplot(janka, aes(x=Density, y=Hardness)) + 
  geom_point()+
  geom_smooth(method=lm)

## Normal least squares linear regression using lm(): 
janka.ls1 <- lm(Hardness~Density, data = janka)
```

Check assumptions for fitting a linear model.
```{r check lm assumptions}
autoplot(janka.ls1)
```


```{r janka box cox}
boxcox(janka.ls1) #see linear regression for janka.ls1
```

Figure 8.1 A likelihood surface for the Box–Cox function applied to the simple linear regression of the Janka wood hardness data.

Box Cox transformations are done by raising data to the power (lambda, $\lambda$) that is systematically varied. For example, when $\lambda$=2, the data are squared, when $\lambda$=0.5 they are square rooted, and so on. The data raised to the power one are untransformed.

Since any number raised to the power of zero equals one, a special behavior is defined that integrates smoothly with the other transformations: $\lambda$=0 is the natural log transformation.

The R output for the `boxcox()` function plots the maximum likelihood
surface (the curve) together with a maximum likelihood-based 95% CI. The 95% CI is produced by dropping down 1.92 log-likelihood units from the maximum likelihood value, moving horizontally left and right (the dotted line labelled 95% in Fig. 8.1) until the likelihood surface is met and then dropping down from these points as shown by the outer vertical dotted lines in the same figure. The value of 1.92 derives from the critical value of chi-squared (deviance, the generalized equivalent of SS, is distributed approximately following chi-square) with 1 DF and at the P < 0.05 level of confidence in question, divided by two (for a two-tailed test).

![](images/boxcox.jpeg)  
\
The results of the Box–Cox transformation suggest that a value of
lambda of around 0.5 will give the best fit to the data so we can proceed using the square-root transformation, or rather the GLM equivalent.

**Basics of Generalized Linear Models**
\
\
The general form of the GLM is:

**glm**(formula, **family**=familytype(**link**=linkfunction), **data**=)
\
\
\
GLMs were first proposed in an article by Nelder and Wedderburn in 1972 (J. R. Statist.
Soc. A, 135: 370). The 1983 book Generalized linear models by McCulloch and Nelder is the standard reference. 

GLMs have three components:

* a linear predictor

* a variance function

* a link function.

The first is the most familiar. In R, the **linear predictor** is what comes after the tilde (~) in our linear model formula. In the Janka example above it is wood density. 

The **variance function** models the variation in the data. This is also familiar to us since OLS uses the normal distribution to model the residual variation. The difference here is that GLMs are not restricted to the normal distribution but make use of a much wider range of distributions including the *poisson*, the *binomial*, and the *gamma*. 

The third component is the least familiar. The **link** function plays a role equivalent to the transformation in normal least squares models. However, *rather than transforming the data we transform the predictions made by the linear predictor*. Commonly used link functions include the *log*, *square root*, and *logistic*.

![](images/GLM_families.jpeg) 

To see how this works, lets fit the Janka model using both OLS and GLM.
```{r janka OLS}
janka.ls1 <- lm(Hardness~Density, data= janka )
anova(janka.ls1)
```
We can fit the same model using the GLM function:
```{r janka GLM}
janka.ml1 <- glm(Hardness~Density, data= janka, family= gaussian(link="identity"))
anova(janka.ml1)
```

Notice in the model specification above, the *family=gaussian* and *link=identity* are the defaults. Like other R code, we can omit specification of default parameters, but probably best to include them until we understand the modes fully.

*Gaussian* is another word for 'normal'; the *identity* link is equivalent to performing no transformation, so no transformation is applied to the fitted values from the linear predictor.

Notice that when using the `anova()` function, we get different information for *lm* vs *glm* model specification. ANOVA of the lm gives a least squares ANOVA table, whereas the glm gives a maximum likelihood analysis of deviance (ANODEV). These outputs look more different than they actually are. 

Recall that for GLMs using the normal (gaussian) distribution, the deviance is the SS (sum of squared residuals). So the values for the deviance here (based on likelihood) match the conventional ANOVA table. However, there is no equivalent of the means squares in the analysis of deviance. 

Quick question: **What are mean squares used for? What do they tell us?**

The point here is to demonstrate that the `glm()` function with the Gaussian variance function and identity link performs the equivalent analysis to the `lm()` function we can see how to take fuller advantage of its greater flexibility.

**We need to model the mean and the variance.** 
We know that in the normal least squares analysis the best fit was produced by the square-root transformation, as recommended by the Box–Cox results. However, with this transformation the variance increased as the mean increased (slightly). We can have the best of both worlds in our GLM by using a square-root link function in combination with a distribution in which the variance increases with the mean. It turns out that for the gamma distribution the variance increases as the square of the mean. We can fit a GLM with a square-root link and gamma distribution variance function as follows:

```{r gamma link function}
janka.gamma <- glm(Hardness~Density, data= janka, family=Gamma(link= "sqrt"))
#summary(janka.gamma)

ggplot(janka,aes(Density,Hardness)) + 
  geom_point() +
  geom_smooth(method="glm",colour="red",
                         method.args=list(family="Gamma"(link="sqrt")))+
  labs(title="GLM, square-root link, Gamma variance")
```
![](images/fig8.3.jpeg)  
Figure 8.3 might help relate the GLM approach using a link function to the linear model analysis of the square-root transformed data.

* To recap, the untransformed data (left-hand panel) show that the positive linear relationship has some upward curvature with increasing scatter. The linear regression of the square-root transformed data (right panel) linearizes the relationship by reducing the larger values more in absolute terms than the smaller values. 

* In contrast, the GLM (right-hand panel) uses a square-root link function to transform the linear relationship on the square-root scale into an upward-bending curve through the middle of the untransformed data.

* The increasing variability is handled separately in the GLM by the gamma distribution variance function. The link function (and its opposite the inverse link function) map the straight line from the linear model and the curve from the GLM onto each other.

We can get CIs for the regression intercept and slope (remember the curve in the figures is linear on the scale of the link function, the square root in this case):

```{r message=FALSE, warning=FALSE}
coef(janka.gamma)
confint(janka.gamma)
```
Since confidence intervals are not based on the normal distribution, they are not constrained to be symmetric. However, they are interpreted in the same way.

**Summary**

* A simple normal least squares linear regression failed to capture the curvature in the relationship and infringed the assumption of approximately equal variability around the regression line: as is often the case the variance increases with the mean. 

* A square-root transformation of the Janka wood hardness data produces a linear relationship but the variability is still unequal. In contrast, log transformation equalizes the variability but generates its own curvature. 

* Instead we use a more flexible GLM approach that can model the mean and variability independently. 

* GLMs based on maximum likelihood use a link function to model the mean (in this case a square-root link) and a variance function to model the variability (in this case the gamma distribution where the variance increases as the square of the mean).

\
\
\
\
\
\
Visual Explanation of Maximum Likelihood:

https://www.youtube.com/watch?v=XepXtl9YKwc


Box-Cox:

https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/






